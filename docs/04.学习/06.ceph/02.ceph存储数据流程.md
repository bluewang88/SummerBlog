---
title: ceph存储数据流程
date: 2021-04-26 18:15:08
permalink: /pages/1fd234/
categories:
  - 学习
  - ceph
tags:
  - ceph
---


本文目标：理解ceph存储流程，例如：当client向ceph集群中写入一个文件时，这个文件是如何存储到ceph中的，其存储过程是如何？

<!-- mroe -->


## ceph存储流程图


![](https://cdn.jsdelivr.net/gh/summerking1/image@main/ceph存储流程图.png)  


## ceph存储流程详解



- File: 就是我们想要存储和访问的文件，这个是面向我们用户的，是我们直观操作的对象。
- Object：object就是Ceph底层RADOS所看到的对象，也就是在Ceph中存储的基本单位。object的大小由RADOS限定（通常为2m或者4m）。
- PG (Placement Group): PG是一个逻辑的概念，它的用途是对object的存储进行组织和位置的映射，通过它可以更好的分配数据和定位数据。
- OSD (Object Storage Device): 它就是真正负责数据存取的服务。


```
graph LR
文件-->对象
对象-->归置组
归置组-->OSD
```

1. 文件到对象的映射

首先，将file切分成多个object，每个object的大小由RADOS限定（通常为2m或者4m）。每个object都有唯一的id即oid，oid由ino和ono产生的

- ino：文件唯一id（比如filename+timestamp）
- ono：切分后某个object的序号(比如0,1,2,3,4,5等)

2. 对象到归置组的映射

对oid进行hash然后进行按位与计算得到某一个PG的id。mask为PG的数量减1。这样得到的pgid是随机的。

注：这与PG的数量和文件的数量有关系。在足够量级的程度上数据是均匀分布的。

3. 归置组到OSD的映射

通过CRUSH算法可以通过pgid得到多个osd，简而言之就是根据集群的OSD状态和存储策略配置动态得到osdid，从而自动化的实现高可靠性和数据均匀分布。在ceph中，数据到底是在哪个osd是通过CRUSH算法计算出来的
