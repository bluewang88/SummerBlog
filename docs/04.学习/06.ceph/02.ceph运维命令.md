---
title: ceph运维命令
date: 2021-03-30 15:23:22
permalink: /pages/a78af9/
categories:
  - 学习
  - ceph
tags:
  - ceph
---


## PG篇

### 1.扩容PG

```
ceph osd pool set {pool-name} pg_num 128
ceph osd pool set {pool-name} pgp_num 128
```


### 2.查看pg，pgp数量

```
[root@summer ~]# ceph osd pool get {pool-name} pg_num
pg_num: 256
[root@summer ~]# ceph osd pool get {pool-name} pgp_num
pgp_num: 256
[root@summer ~]#
```

---

## pool篇


### 1.创建pool

`ceph osd create pool ${poolname} ${pg_num} ${pgp_num}`

```shell
[root@summer ~]# ceph osd  pool create  lalala 128 128
pool 'lalala' created
[root@summer ~]# ceph osd  pool get  lalala pg_num
pg_num: 128
[root@summer ~]# ceph osd  pool get  lalala pgp_num
pgp_num: 128
[root@summer ~]# ceph osd lspools
1 k60,4 lalala,
[root@summer ~]# 

```


### 2.删除pool
```shell
[root@summer ~]# ceph osd pool delete lalala lalala  --yes-i-really-really-mean-it
pool 'lalala' removed
```

### 3.修改pool参数


`ceph osd pool set ${poolname} pg_num ${pg_num}`

`ceph osd pool set ${poolname} pgp_num ${pgp_num}`

- 需要注意, pg_num只能增加, 不能缩小

```shell
[root@summer ~]# ceph osd pool set lalala pg_num 256
set pool 5 pg_num to 256
[root@summer ~]# ceph osd pool set lalala pgp_num 256
Error EBUSY: currently creating pgs, wait
[root@summer ~]# ceph osd pool set lalala pgp_num 256
set pool 5 pgp_num to 256
```


### 4.查看pool

`ceph osd lspools`

`rados lspools`

`ceph osd dump | grep -i pool`


```shell
[root@summer ~]# ceph osd lspools
1 k60,4 lalala,
[root@summer ~]# 
[root@summer ~]# ceph osd pool ls
k60
r-_k38-frp-8974
r-_k60sysmag001
[root@summer ~]# rados df
pool name                 KB      objects       clones     degraded      unfound           rd        rd KB           wr        wr KB
lalala                     0            0            0            0           0            0            0            0            0
k60           37           20            0            0           0        55066        24773           80           78
  total used        84056812           20
  total avail       41690404
  total space      125747216
[root@summer ~]# rados lspools
k60
lalala
[root@summer ~]# ceph osd dump | grep pool
pool 1 'k60' replicated size 2 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 128 pgp_num 128 last_change 20 flags hashpspool crash_replay_interval 45 stripe_width 0
pool 4 'lalala' replicated size 2 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 256 pgp_num 256 last_change 185 flags hashpspool stripe_width 0
```

### 5.查看pool的副本数

- 也可以理解为元数据保护级别

```shell
[root@summer ~]# ceph osd pool set k60 size 3
set pool 1 size to 3
[root@summer ~]# ceph osd pool get k60 size
size: 3
```

## osd篇
### 1.查看osd权重（REWEIGHT）

```shell
[root@summer ~]# ceph osd df
ID WEIGHT  REWEIGHT SIZE   USE    AVAIL  %USE  VAR  
 0 0.03000  1.00000 30700M 20535M 10164M 66.89 1.00 
 1 0.03000  1.00000 30700M 20555M 10144M 66.96 1.00 
 2 0.03000  1.00000 30700M 20543M 10156M 66.92 1.00 
 3 0.03000  0.86485 30700M 20535M 10164M 66.89 1.00 
              TOTAL   119G 82170M 40629M 66.91      
MIN/MAX VAR: 1.00/1.00  STDDEV: 0.03
[root@summer ~]# ceph osd stat
     osdmap e158: 4 osds: 4 up, 4 in
[root@summer ~]# ceph osd tree
ID WEIGHT  TYPE NAME               STATUS REWEIGHT PRIMARY-AFFINITY 
-1 0.12000 root default                                             
-2 0.06000     host iscloud163-201                                  
 0 0.03000         osd.0               up  1.00000          1.00000 
 1 0.03000         osd.1               up  1.00000          1.00000 
-3       0     host summer                                  
-4 0.06000     host iscloud163-197                                  
 2 0.03000         osd.2               up  1.00000          1.00000 
 3 0.03000         osd.3               up  0.86485          1.00000 
[root@summer ~]# 
```
